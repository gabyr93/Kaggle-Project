---
title: "EDA HOMECREDIT"
author: "Gaby Rodriguez"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false

---


```{r, data import and inspections, echo = FALSE, warning=FALSE, message=FALSE}
# Libraries
library(car)
library(reshape2)
library(data.table)
library (ggvenn)
library(ggVennDiagram)
library(caret)
library(corrplot)
library(data.table)
library(dplyr)
library(ggcorrplot)
library(ggplot2)
library(ggthemes)
library(janitor)
library(knitr)
library(naniar)
library(patchwork)
library(psych)
library(dplyr)
library(skimr)
library(tidyverse)
library(tidyr)
library(corrplot)

dir<- getwd()
setwd(dir)
```


# Introduction
Home Credit is a company that focuses on making financial services more accessible for people who donâ€™t have traditional credit histories, often referred to as the unbanked. Their goal is to give fair opportunities to those who are often excluded from borrowing. One of their main challenges is figuring out which clients are likely to repay their loans. In this project, weâ€™ll use the data to build predictive models that estimate the chance of default. These insights will help Home Credit make smarter lending decisions, reduce financial risk, and give qualified clients a better chance to succeed.

## Analytic problem
The main purpose of the Exploratory Data Analysis is to explore the daset in order to understand patterns, trends and identify what predictors are relevant to take in consideration to identify the customer's ability to repay the loan,for this project we identify that the target variable is the prediction of whether a client will have trouble with loan repayment. Good-standing clients are represented by (0), but clients who have defaulted or made one or more late installments are represented by (1). This corresponds to a binary classification analysis for supervised learning.

This EDA report will provide a comprehensive overview of the dataset, beginning with key variables and their descriptions, followed by data cleaning and preprocessing steps such as handling missing values and outliers. It will then present summary statistics and distributions of both numerical and categorical features, along with correlation analysis to identify relationships between variables and an examination of potential biases or inconsistencies in the data. The insights gained will inform the development of a robust predictive model that minimizes risk while optimizing financial inclusion. By the end of this analysis, we expect to identify the most relevant features influencing client repayment behavior, detect and address any data anomalies, generate descriptive statistics and visualizations to support data-driven decision-making, and establish a foundation for feature engineering and predictive modeling. Ultimately, the findings will serve as a crucial step in building a machine learning model that improves Home Creditâ€™s risk assessment and enhances its ability to provide responsible lending solutions.

### High Level Risk
- Class imbalance
- Missing data
- Mixed data types
- potential data leakage

# Data Overview
The Home Credit dataset has 307,511 clients and 122 variables, with the target showing whether a client had repayment issues about 8% or stayed in good standing 92%. It includes demographic details like gender, age, family status, and number of children; financial and employment information such as income, occupation, and days employed; and loan details like contract type, credit amount, annuity, and goods price. There are also features on housing and property, a lot of binary flags for documents and phone ownership, external credit scores, and credit bureau inquiries. Even though itâ€™s a really detailed dataset, it comes with some challenges, many of the property related variables have more than 60% missing values. The target variable in this dataset is TARGET, which tells us whether a client had trouble repaying their loan. A value of 0 means the client stayed in good standing and did not default, while a value of 1 means the client did experience repayment difficulties.

### Datasets
Aapplication_train|test.csv serves as the primary table, containing 122 variables that display client details from the loan application. It has already been separated into training and testing sets. Other datasets include credit_card_balance, installments_payments, and previous_application, which provide detailed insights into an applicant's earlier loans with Home Credit.

```{r, Import Data}
#Import data
HomeCredit <- read.csv("C:/Users/gabri/OneDrive/Documentos/GABY 2024/master/Capstone_2/home-credit-default-risk/application_train.csv", stringsAsFactors = TRUE)
```

```{r, Convert Target and other variables into Factor}
#Convert Target and other variables into Factors
HomeCredit <- HomeCredit %>%
  mutate(
    TARGET = as.factor(TARGET),   # Convert target variable to factor for classification
    across(
      c(FLAG_DOCUMENT_2:FLAG_DOCUMENT_21,  
        FLAG_MOBIL, FLAG_EMP_PHONE, FLAG_WORK_PHONE,  
        FLAG_CONT_MOBILE, FLAG_PHONE, FLAG_EMAIL,     
        REG_REGION_NOT_LIVE_REGION, REG_REGION_NOT_WORK_REGION, LIVE_REGION_NOT_WORK_REGION,  
        REG_CITY_NOT_LIVE_CITY, REG_CITY_NOT_WORK_CITY, LIVE_CITY_NOT_WORK_CITY),             
      as.factor) )
```



```{r, Overview HomeCredit dataset,  echo = FALSE, warning=FALSE, message=FALSE}

# Overview of the HomeCredit dataset

# Check structure of the dataset
glimpse(HomeCredit) 

# Summary statistics for all variables
summary(HomeCredit)  

# Distribution of target variable
prop.table(table(HomeCredit$TARGET))  


```


### Numerical Data Analysis

#### Which predictors show the strongest univariate association with target?


  - External credit scores (EXT_SOURCE_1/2/3): Higher scores strongly reduce default risk.

  - Age (DAYS_BIRTH): Older clients are less likely to default.

  - Employment length (DAYS_EMPLOYED): Longer, stable employment lowers risk.

  - Income (AMT_INCOME_TOTAL): Higher income reduces default odds.

  - Credit/annuity amounts: Larger loans raise risk if not supported by income.

  - Loan type (NAME_CONTRACT_TYPE): Revolving loans show higher default risk than cash loans.

  - **Takeaway:** Credit bureau scores, age, employment stability, and income-loan ratios stand out as the most influential single variables tied to repayment problems.





**External Credit Scores and Default Risk**

```{r  ,warning=FALSE, message=FALSE}
ggplot(HomeCredit, aes(x = TARGET, y = EXT_SOURCE_2, fill = TARGET)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = c("0" = "lightblue", "1" = "pink"),
                    labels = c("0" = "No repayment issues", "1" = "Repayment issues")) +
  labs(title = "Credit Score by Repayment Status",
       y = "EXT_SOURCE_2", x = "Target", fill = "Repayment Status") +
  theme_minimal()

```
The plot shows that clients without repayment issues (Target = 0) have higher external credit scores on average compared to those with repayment issues (Target = 1). The median score for non-defaulters is visibly higher, and the spread of scores for defaulters skews lower. This confirms that external credit scores are strong predictors of default risk.


**Age Distribution by Repayment Status**
```{r}
HomeCredit %>%
  mutate(AGE_YEARS = abs(DAYS_BIRTH) / 365) %>%
  ggplot(aes(x = TARGET, y = AGE_YEARS, fill = TARGET)) +
  geom_boxplot(alpha = 0.7) +
  scale_fill_manual(values = c("0" = "lightblue", "1" = "pink"),
                    labels = c("0" = "No repayment issues", "1" = "Repayment issues")) +
  labs(title = "Age Distribution by Repayment Status",
       y = "Age (years)", x = "Target", fill = "Repayment Status") +
  theme_minimal()

```
Applicants with repayment issues are typically younger, while older clients show a lower likelihood of default.

**Employment Length and Default Risk**
```{r, warning=FALSE, message=FALSE}
HomeCredit %>%
  mutate(DAYS_EMPLOYED = na_if(DAYS_EMPLOYED, 365243),
         EMP_YEARS = abs(DAYS_EMPLOYED) / 365) %>%
  ggplot(aes(x = TARGET, y = EMP_YEARS, fill = TARGET)) +
  geom_violin(trim = TRUE, alpha = 0.6) +
  scale_fill_manual(values = c("0" = "lightblue", "1" = "pink"),
                    labels = c("0" = "No repayment issues", "1" = "Repayment issues")) +
  labs(title = "Employment Length by Repayment Status",
       y = "Employment (years)", x = "Target", fill = "Repayment Status") +
  theme_minimal()

```
Borrowers with repayment issues tend to have shorter employment histories, while those with longer, stable employment are less likely to default.

**Income Levels by Repayment Status**


```{r}
HomeCredit %>%
  ggplot(aes(x = as.factor(TARGET), y = AMT_INCOME_TOTAL, fill = as.factor(TARGET))) +
  geom_boxplot(alpha = 0.7, outlier.alpha = 0.3) +
  scale_y_log10() +
  scale_fill_manual(values = c("0" = "lightblue", "1" = "pink"),
                    labels = c("0" = "No repayment issues", "1" = "Repayment issues")) +
  labs(title = "Distribution of Client Income by Repayment Status",
       x = "Target",
       y = "Total Income (log scale)",
       fill = "Target") +
  theme_minimal()
```

**How imbalanced is target?**

- 91% of clients repay without incurring penalties, indicating an unequal distribution of the target variable. An imbalance towards the negative class is shown by about 25,000 positive classifications (Repayment Issues) out of 300,000(No Repayment Issues).

- Because this dataset is highly imbalanced, metrics like Area Under the Receiver Operating Characteristic Curve will be used to determine model performance as it is more robust to skewed data sets.


## Missing Data
 
#### **Where are the missing values concentrated, and how might we handle them ?**


```{r, Missing Values}

# Summarize missing data for each variable in the HomeCredit dataset
miss_summary <- miss_var_summary(HomeCredit) %>% 
  # Arrange variables in descending order of percent missing
  arrange(desc(pct_miss))

# Display the top 25 variables with the highest percentage of missing values
head(miss_summary, 25)

```

Most missing values are concentrated in apartment and household related variables (COMMONAREA, NONLIVINGAPARTMENTS, LIVINGAPARTMENTS), each with 68â€“70% missing, making them unreliable.

### Cleaning Data

1. **Drop** variables with >50% missing; **keep** those with â‰¤50%.  

2. **Exception:** Keep `EXT_SOURCE_1` (56% missing) because it is a strong predictor of repayment risk; will impute later.

3. **Remove** other near-50% household structure fields not directly relevant to repayment. 

```{r, Cleaning Data}

# % missing per variable (full dataset)
missing_perc <- colMeans(is.na(HomeCredit))

# Keep rule: â‰¤ 50% missing
cols_to_keep <- names(missing_perc[missing_perc <= 0.50])

# Exception: always keep EXT_SOURCE_1
cols_final <- unique(c(cols_to_keep, "EXT_SOURCE_1"))

# Filter dataset
HomeCredit_filtered <- HomeCredit %>%
  dplyr::select(all_of(cols_final))

#Report kept vs dropped (counts and percentages)
n_total <- ncol(HomeCredit)
n_kept  <- ncol(HomeCredit_filtered)
n_drop  <- n_total - n_kept
cat(sprintf("Kept %d/%d (%.1f%%); Dropped %d (%.1f%%)\n",
            n_kept, n_total, 100*n_kept/n_total, n_drop, 100*n_drop/n_total))

# Summary of remaining missingness (filtered dataset)
miss_summary_filtered <- naniar::miss_var_summary(HomeCredit_filtered) %>%
  dplyr::arrange(dplyr::desc(pct_miss))

head(miss_summary_filtered, 20)


```

#### Interpretation
Out of 122 total variables, 85 were retained and 37 were removed. The highest missingness occurred in household-related features; Such as apartment size, number of floors, and common areas, where approximately 68â€“70% of values were blank. Because applicants rarely report these details, the information is highly inconsistent. Given this level of missingness, imputing these features would introduce more noise than signal, so they were excluded from the analysis.


### Variables Excluded
```{r, Dropped Data}
# Packages
library(dplyr)
library(ggplot2)
library(naniar)
library(stringr)

# Identify dropped columns 
dropped_cols <- setdiff(names(HomeCredit), names(HomeCredit_filtered))

# Missingness summary on the dataset
dropped_summary <- miss_var_summary(HomeCredit) %>%
  filter(variable %in% dropped_cols) %>%
  arrange(desc(pct_miss)) %>%
  mutate(
    variable = as.character(variable),  
    pct_miss = as.numeric(pct_miss)
  )

#Heatmap
ggplot(dropped_summary,
       aes(x = 1, y = reorder(variable, pct_miss), fill = pct_miss)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "#fcbfdb", high = "#d63384") +
  labs(
    title = "Dropped Columns Heatmap",
    x = "",
    y = "Dropped Column",
    fill = "% Missing"
  ) +
  scale_y_discrete(labels = ~ str_wrap(., width = 30)) + 
  theme_minimal(base_size = 10) +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )


```
Most of the columns we dropped are home or apartment details (e.g., floors, area, common spaces). These fields are missing for many applicants, so rather than guess, we removed them.
Data is still missing from certain columns, and those gaps must be imputed.Prior to imputing the missing data, let's examine the remaining variables more closely. 

### Variables Retained with Missingness

```{r, Missing Data}
# Identify columns with missing values
na <- colSums(is.na(HomeCredit_filtered))

# Filter only columns that have NA values
na <- na[na > 0]

# Display the columns with their count of missing values
print(na)

```

### Interpreting Missing Data in Retained Variables

::: {.callout-note title="Important"}

Even after filtering, some of the retained variables still contain missing values.  

- **Minimal missingness:** `CNT_FAM_MEMBERS`, `AMT_ANNUITY`, `AMT_GOODS_PRICE`, `DAYS_LAST_PHONE_CHANGE` â€” few gaps; simple imputation.  

- **Moderate missingness:** `EXT_SOURCE_2`, `OBS/DEF_*_SOCIAL_CIRCLE` â€” manageable; imputation feasible.  

- **Large gaps:** `EXT_SOURCE_1`, `EXT_SOURCE_3`, `TOTALAREA_MODE` â€” important predictors; retain with careful imputation.  

- **Credit bureau request variables:** (`AMT_REQ_CREDIT_BUREAU_*`) â€” 41,000 missing values likely mean no credit history; encode as zero or separate category.  
 

**Summary:** Some variables can be handled with simple fixes, while others require targeted strategies to preserve predictive value.
:::

## Outliers

### Overall Extreme Outliers

```{r, Outliers}
# Function to detect extreme outliers using 3Ã— IQR
ExtremeOutliers <- function(df) {
  ExtremeOutliersList <- list()  

  for (col in names(df)) {
    if (is.numeric(df[[col]])) {  # Only check numeric columns
      Q1 <- quantile(df[[col]], 0.25, na.rm = TRUE)
      Q3 <- quantile(df[[col]], 0.75, na.rm = TRUE)
      IQR_value <- Q3 - Q1

      # Define extreme bounds (3Ã— IQR rule)
      lower_bound <- Q1 - 3 * IQR_value
      upper_bound <- Q3 + 3 * IQR_value

      # Identify extreme outliers
      outliers <- which(df[[col]] < lower_bound | df[[col]] > upper_bound)

      if (length(outliers) > 0) {
        ExtremeOutliersList[[col]] <- outliers  # Store outlier indices
      }
    }
  }
  return(ExtremeOutliersList)
}

# Apply function
extreme_outliers <- ExtremeOutliers(HomeCredit_filtered)

extreme_summary <- tibble(
  Variable = names(extreme_outliers),
  N_Outliers = sapply(extreme_outliers, length)
)

knitr::kable(extreme_summary, caption = "Summary of Extreme Outliers per Variable")



```

Overall, thereâ€™s a clear split: some variables only have a small number of extreme values, while others are heavily skewed with large outlier counts. The smaller cases wonâ€™t have much impact, but the variables with massive outliers will need closer attention. For modeling, applying transformations; e.g., log-scale or handling those extremes as a separate group will help prevent them from biasing the results.

### Check for Key Variables

```{r }
# Function for a single variable (IQR rule)
find_extreme_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)  
  Q3 <- quantile(x, 0.75, na.rm = TRUE)  
  IQR_value <- Q3 - Q1  

  lower_bound <- Q1 - 10 * IQR_value
  upper_bound <- Q3 + 8 * IQR_value

  return(which(x < lower_bound | x > upper_bound))
}

# Find extreme outliers for specific variables
extreme_income_outliers   <- find_extreme_outliers(HomeCredit_filtered$AMT_INCOME_TOTAL)
extreme_children_outliers <- find_extreme_outliers(HomeCredit_filtered$CNT_CHILDREN)
extreme_age_outliers      <- find_extreme_outliers(HomeCredit_filtered$DAYS_BIRTH)

# Print counts
cat("Number of extreme outliers in AMT_INCOME_TOTAL:", length(extreme_income_outliers), "\n")
cat("Number of extreme outliers in CNT_CHILDREN:", length(extreme_children_outliers), "\n")
cat("Number of extreme outliers in DAYS_BIRTH:", length(extreme_age_outliers), "\n")

# Check for number of extreme outliers in the AMT_INCOME column:

extreme_income_outliers
median_income <- median(HomeCredit$AMT_INCOME_TOTAL)
cat("The median in AMT_INCOME_TOTAL is :", median_income)
```
The highest value in the AMT_INCOME column is 117,000,000, which is most likely a mistake. The median income is 147,150, which makes a lot more sense. Since that incorrect maximum was already removed, it doesnâ€™t show up in the remaining outliers. After reviewing the rest of the income outliers, the values seem reasonable, so thereâ€™s no need to remove them.

### Findings on Extreme Outliers

- AMT_INCOME_TOTAL: About 276 extreme cases remain, mostly very high earners. These can be genuine but rare clients; applying a log transformation will reduce their influence.

- CNT_CHILDREN: Only 10 extreme cases, such as households with more than 10 children.

- DAYS_BIRTH: No extreme outliers detected, this variable is clean and reliable.

- DAYS_EMPLOYED: Over 59,000 cases flagged, driven by the placeholder value 365243, meaning â€œno employmentâ€. This should be recoded as missing.

- REGION_RATING_CLIENT / W_CITY: Around 78kâ€“80k outliers; distribution is highly skewed. These may require grouping into broader categories instead of leaving as raw values.

- Credit Bureau Requests: Ranges from 43kâ€“50k extreme cases, suggesting reporting spikes. These should be binned or capped to prevent extreme counts from dominating.

- Overall: A few variables: income, children and age have manageable outliers that can be transformed, while others like employment, region ratings and bureau requests have systematic issues that need correction or recoding before modeling.

## Categorical Data Analysis

```{r }
# Ensure TARGET is a factor
HomeCredit_filtered$TARGET <- factor(HomeCredit_filtered$TARGET, levels = c(0, 1))

# Plot gender vs target
ggplot(HomeCredit_filtered, aes(x = CODE_GENDER, fill = TARGET)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = c("0" = "#d63384", "1" = "#e0e0e0"),
                    labels = c("Good Standing", "Default")) +
  theme_minimal() +
  labs(title = "Loan Repayment by Gender", x = "Gender", y = "Count", fill = "Repayment Status")

```

The analysis shows that most clients in the dataset are women, followed by men, While women make up a larger share of borrowers, the proportion of defaults compared to clients in good standing is similar across genders. This suggests that gender does not strongly influence repayment outcomes.


```{r }
# Loan Repayment by Family Status

ggplot(HomeCredit_filtered, aes(x = NAME_FAMILY_STATUS, fill = TARGET)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = c("0" = "#d63384", "1" = "#e0e0e0"),
                    labels = c("Good Standing", "Default")) +
  theme_minimal() +
  labs(title = "Loan Repayment by Family Status",
       x = "Family Status", y = "Count", fill = "Repayment Status") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))
```
The majority of clients in the dataset are married, followed by single and civil marriage categories. Across all family statuses, most borrowers remain in good standing, while defaults make up only a smaller share. Although married applicants dominate in count, the proportion of defaults appears fairly consistent across categories, suggesting family status does not strongly impact repayment outcomes.

```{r }
# Loan Repayment by Education

ggplot(HomeCredit_filtered, aes(x = NAME_EDUCATION_TYPE, fill = TARGET)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = c("0" = "#d63384", "1" = "#e0e0e0"),
                    labels = c("Good Standing", "Default")) +
  theme_minimal() +
  labs(title = "Loan Repayment by Education Level",
       x = "Education Type", y = "Count", fill = "Repayment Status") +
  theme(axis.text.x = element_text(angle = 30, hjust = 1))

```


Most clients in the dataset have either secondary or higher education, with secondary education being the largest group. Defaults are present across all education levels but remain a smaller share compared to clients in good standing. The distribution suggests that while education level reflects borrower demographics, it does not show a dramatic difference in repayment outcomes.

```{r }
# Loan Repayment by Occupation

ggplot(HomeCredit_filtered, aes(x = OCCUPATION_TYPE, fill = TARGET)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = c("0" = "#d63384", "1" = "#e0e0e0"),
                    labels = c("Good Standing", "Default")) +
  theme_minimal() +
  labs(title = "Loan Repayment by Occupation",
       x = "Occupation Type", y = "Count", fill = "Repayment Status") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
Most borrowers fall into labor-related jobs, particularly laborers, sales staff, and core staff, with smaller groups in roles like managers, drivers, and accountants. While defaults are visible across occupations, they remain much lower than the number of clients in good standing. The distribution shows that repayment issues occur in many job types but do not appear to be heavily concentrated in one specific occupation


```{r, warning=FALSE, message=FALSE}
# Loan Repayment by Contract Type

ggplot(HomeCredit_filtered, aes(x = NAME_CONTRACT_TYPE, fill = TARGET)) +
  geom_bar(position = "dodge") +
  scale_fill_manual(values = c("0" = "#d63384", "1" = "#e0e0e0"),
                    labels = c("Good Standing", "Default")) +
  theme_minimal() +
  labs(title = "Loan Repayment by Contract Type",
       x = "Contract Type", y = "Count", fill = "Repayment Status")

```
Most clients in the dataset applied for cash loans, while a smaller share took revolving loans. Across both contract types, the majority of clients are in good standing, though defaults are more common among cash loan borrowers simply because they make up most of the applications. Overall, contract type shows differences in volume but does not appear to be a major driver of repayment risk.

## Imputation

```{r, warning=FALSE, message=FALSE}
df <- HomeCredit_filtered

# Fix no employment and set to NA before imputation
if ("DAYS_EMPLOYED" %in% names(df)) {
  df$DAYS_EMPLOYED[df$DAYS_EMPLOYED == 365243] <- NA
}

# columns that still have NAs
na_cols <- names(df)[colSums(is.na(df)) > 0]

# Impute numeric columns with median
num_cols <- na_cols[sapply(df[na_cols], is.numeric)]
for (col in num_cols) {
  med <- median(df[[col]], na.rm = TRUE)
  df[[col]][is.na(df[[col]])] <- med
}

# Impute categorical columns (factor/character) with mode
is_cat <- function(x) is.factor(x) || is.character(x)
ModeVal <- function(x) {
  x <- x[!is.na(x)]
  if (length(x) == 0) return(NA)
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
cat_cols <- na_cols[sapply(df[na_cols], is_cat)]
for (col in cat_cols) {
  m <- ModeVal(df[[col]])
  # ensure the mode exists in factor levels before assignment
  if (is.factor(df[[col]]) && !is.na(m) && !(m %in% levels(df[[col]]))) {
    levels(df[[col]]) <- c(levels(df[[col]]), m)
  }
  df[[col]][is.na(df[[col]])] <- m
}

# Fill missing with 0 to represent no credit history
ext_cols <- grep("^EXT_SOURCE", names(df), value = TRUE)
if (length(ext_cols)) {
  df <- df %>%
    mutate(across(all_of(ext_cols), ~ tidyr::replace_na(., 0)))
}

HomeCredit_imputed <- df

#Check data after imputation 

cat("Any NAs left overall? ", sum(is.na(HomeCredit_imputed)), "\n")
if (length(ext_cols)) {
  print(sapply(ext_cols, function(v) sum(is.na(HomeCredit_imputed[[v]]))))
}
if ("DAYS_EMPLOYED" %in% names(HomeCredit_imputed)) {
  cat("NAs in DAYS_EMPLOYED after sentinel fix: ",
      sum(is.na(HomeCredit_imputed$DAYS_EMPLOYED)), "\n")
}


```
#### Imputation Summary

After applying the imputation strategy, the dataset no longer contains any missing values. This confirms that the cleaning pipeline was effective. Key points include:

- **Numeric Variables:** All gaps in income, annuity, goods price, and family size were successfully filled using median imputation, ensuring robustness against skewed distributions.

- **Categorical Variables:** Factors such as family status, occupation, and housing type were completed with the most frequent category, maintaining consistency with observed distributions.

- **Credit Bureau Scores:** EXT_SOURCE_1, EXT_SOURCE_2, and EXT_SOURCE_3â€”which are strong predictors, were imputed with 0 to indicate no credit history. This prevents them from being dropped while preserving their interpretive value.

- **Employment Variable:** The placeholder value of 365243 in DAYS_EMPLOYED was recoded as missing and then imputed, ensuring unemployed clients are not treated as extreme outliers.

- **Verification:** Checks confirm that all EXT_SOURCE variables and DAYS_EMPLOYED now contain no NAs, and the dataset is fully imputed.


### Correlation

```{r correlation-highlighted, fig.width=11, fig.height=9, message=FALSE, warning=FALSE}

# Select numeric variables
numeric_data <- HomeCredit_filtered %>%
  select(where(is.numeric))

# Compute correlation matrix
corr_matrix <- cor(numeric_data, use = "pairwise.complete.obs")

# Convert matrix to long format safely for data.table
melted_corr <- as.data.table(as.table(corr_matrix))  # Safe alternative to melt(matrix)
setnames(melted_corr, c("Var1", "Var2", "value"))

# Define your key variables
highlight_vars <- c(
  "EXT_SOURCE_1", "EXT_SOURCE_2", "EXT_SOURCE_3",
  "FLOORSMAX_MODE", "FLOORSMIN_MODE", "YEARS_BEGINEXPLUATATION_MODE", "TOTALAREA_MODE",
  "OBS_30_CNT_SOCIAL_CIRCLE", "DEF_30_CNT_SOCIAL_CIRCLE", 
  "OBS_60_CNT_SOCIAL_CIRCLE", "DEF_60_CNT_SOCIAL_CIRCLE"
)

# Plot
ggplot(melted_corr, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "#7b3294",   # purple
    mid = "white",     
    high = "#d63384",  # dark pink
    midpoint = 0,
    limit = c(-1, 1),
    name = "Correlation"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    axis.text.x = element_text(angle = 60, hjust = 1, size = 8,
      colour = ifelse(levels(factor(melted_corr$Var1)) %in% highlight_vars, "red", "black")),
    axis.text.y = element_text(size = 8,
      colour = ifelse(levels(factor(melted_corr$Var2)) %in% highlight_vars, "red", "black")),
    panel.grid = element_blank()
  ) +
  labs(
    title = "Correlation Heatmap",
    x = "", y = ""
  )
```

#### Interpretation of the Correlation Heatmap
The heatmap shows the relationships among all numeric variables in the dataset. Strong positive correlations appear as darker pink, while weaker or negative relationships are shown in purple. Variables labeled in red highlight those with especially high correlations based on known domain importance or statistical strength.

Several clusters stand out:

- External credit scores (EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3) are highly correlated with each other.

- Housing and property features (FLOORSMAX, FLOORSMIN, YEARS_BEGINEXPLUATATION, TOTALAREA_MODE) show strong internal correlations, reflecting shared building-related information.

- Social circle variables (OBS_30_CNT_SOCIAL_CIRCLE, DEF_30_CNT_SOCIAL_CIRCLE, OBS_60_CNT_SOCIAL_CIRCLE, DEF_60_CNT_SOCIAL_CIRCLE) form another tightly related group.

This clustering gives insight into potential feature redundancy. Highly correlated predictors may increase multicollinearity in linear models and unnecessarily inflate model complexity.

#### Correlation with target Variable

```{r, warning=FALSE, message=FALSE}
### Correlation with TARGET Variable

# Convert TARGET to numeric (0/1)
df_corr <- HomeCredit_imputed
df_corr$TARGET <- as.numeric(as.character(df_corr$TARGET))

# Compute correlations
target_corr <- cor(df_corr %>% dplyr::select(where(is.numeric)),
                   df_corr$TARGET, use = "pairwise.complete.obs")

# Sort by absolute correlation
target_corr <- sort(target_corr[,1], decreasing = TRUE)

# Build tidy table
corr_table <- data.frame(
  Variable = names(target_corr)[2:16],
  Correlation = round(target_corr[2:16], 3)
)

# Display without row names
kable(corr_table, caption = "Top Correlations with TARGET", row.names = FALSE)


```

The strongest predictor of default is DAYS_BIRTH (0.08), showing that younger applicants are more likely to default. Regional ratings (0.06) also matter, with clients from lower-rated cities and regions facing higher risk. Recent changes in personal data like phone number or ID publish date (0.05) signal instability and increase default likelihood. Other features such as social circle defaults and credit bureau requests show weaker effects (<0.03). Overall, the correlations are modest, confirming that no single variable explains default strongly, but age, region, and recent changes are the most relevant signals.


# AI Usage Log

## My promt in ChatGpt 5

"I have a project and I'm working on the EDA, this project is for Home Credit Default Risk, for more information, here is the Link :
https://www.kaggle.com/competitions/home-credit-default-risk/overview

Specially for the EDA , Im using the training data 

Based on these description, which 10 columns would you recommend as most predictive of whether an applicant will default (TARGET)? Provide your insights.

After, you provide this, please, compare your predictors vs mine in a ven diagram "

**ðŸ”Ÿ Recommended Predictive Columns for Default Risk (TARGET)**

Based on the Kaggle Home Credit Default Risk description and standard EDA findings, the following 10 variables are typically the most predictive of loan default:

- EXT_SOURCE_1 â€“ External credit score (from third-party bureau).

- EXT_SOURCE_2 â€“ Strongest single predictor; higher score â†’ lower default risk.

- EXT_SOURCE_3 â€“ Another external score highly correlated with repayment ability.

- DAYS_BIRTH â€“ Applicantâ€™s age (negative values; older clients default less often).

- DAYS_EMPLOYED â€“ Length of employment (longer stability â†’ lower risk, though watch for outliers like 365243 â€œnot employedâ€).

- AMT_CREDIT â€“ Total credit amount requested. Large amounts relative to income raise risk.

- AMT_ANNUITY â€“ Loan annuity; repayment burden predictor.

- AMT_GOODS_PRICE â€“ Goods purchased with credit; important in relation to income.

- NAME_CONTRACT_TYPE â€“ Cash vs revolving loans (revolving loans have higher default risk).

- REGION_RATING_CLIENT / REGION_RATING_CLIENT_W_CITY â€“ Proxy for region-level economic risk (higher rating = lower default risk).

**ðŸ‘‰ Why these?**

- The external sources dominate predictive power (validated in Kaggle solutions).

- Demographics (age, employment) capture borrower reliability.

- Loan structure variables (credit, annuity, goods price, loan type) capture repayment burden.

- Regional ratings act as risk proxies.


**âœ… Your Predictors in the EDA Report**

From your uploaded HTML, your EDA identified similar strong predictors:

- External credit scores (EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3)

- EDA_HOMECREDIT

- Age (DAYS_BIRTH)

- EDA_HOMECREDIT

- Employment stability (DAYS_EMPLOYED)

- EDA_HOMECREDIT

- Income (AMT_INCOME_TOTAL)

- EDA_HOMECREDIT

- Credit/annuity amounts (AMT_CREDIT, AMT_ANNUITY)

- EDA_HOMECREDIT

- Loan type (NAME_CONTRACT_TYPE)

- EDA_HOMECREDIT

- You emphasized income directly, whereas I emphasized regional ratings as a differentiator.

**ðŸ”„ Comparison: Venn Diagram**

Hereâ€™s the overlap between your predictors and my recommended set:

** Overlap (both sets agree):**

- EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3

- DAYS_BIRTH

- DAYS_EMPLOYED

- AMT_CREDIT

- AMT_ANNUITY

- NAME_CONTRACT_TYPE

Only in Your Set:

- AMT_INCOME_TOTAL

Only in My Set:

- AMT_GOODS_PRICE

- REGION_RATING_CLIENT / REGION_RATING_CLIENT_W_CITY

```{r }
#List my predictors
my_predictors <- c("EXT_SOURCE_1","EXT_SOURCE_2","EXT_SOURCE_3",
                     "DAYS_BIRTH","DAYS_EMPLOYED","AMT_INCOME_TOTAL",
                     "AMT_CREDIT","AMT_ANNUITY","NAME_CONTRACT_TYPE")

#List AI predictors
ai_predictors <- c("EXT_SOURCE_1","EXT_SOURCE_2","EXT_SOURCE_3",
                   "DAYS_BIRTH","DAYS_EMPLOYED","AMT_CREDIT","AMT_ANNUITY",
                   "NAME_CONTRACT_TYPE","AMT_GOODS_PRICE","REGION_RATING_CLIENT")

venn_list <- list(Mine = my_predictors, AI = ai_predictors)

#Draw Ven Diagram
ggVennDiagram(venn_list, label_alpha = 0) +
  scale_fill_gradient(low = "white", high = "pink") +
  labs(title = "Data-Informed vs AI-Informed")




```
The Venn diagram shows that most of my chosen predictors match with the AIâ€™s list, with 8 variables (73%) overlapping. These include the external credit scores (EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3), age (DAYS_BIRTH), employment history (DAYS_EMPLOYED), and loan details like AMT_CREDIT, AMT_ANNUITY, and NAME_CONTRACT_TYPE. The AI added two extra predictors: AMT_GOODS_PRICE and REGION_RATING_CLIENTâ€”which give more context about the value of the loan and the borrowerâ€™s region. My list included AMT_INCOME_TOTAL, since I saw income as an important factor. Overall, this shows that both approaches agree on the main drivers of default risk, and the small differences suggest extra features that could make the model stronger.



# Transactional Data

Explore Bureau data

```{r, echo=FALSE}
# Load the bureau.csv file
bureau <- read.csv("C:/Users/gabri/OneDrive/Documentos/GABY 2024/master/Capstone_2/home-credit-default-risk/bureau.csv", stringsAsFactors = TRUE)

glimpse (bureau)
summary(bureau)

```
The bureau.csv file has more than 1.7M records that capture clientsâ€™ past and current credits reported by external bureaus. Most entries are either Closed or Active, and the majority are consumer loans or credit cards, while mortgages and other loan types appear less often. Loan amounts range a lotâ€”the median is around 125k, but there are extreme outliers in the hundreds of millions. Debt and overdue balances are usually zero, though some clients carry big obligations that could point to higher risk. The time variables show that most bureau records go back a few years before the loan application, which helps add context on repayment history. Some fields, like annuity amounts and end dates, have a lot of missing data that will need to be addressed. Overall, this dataset gives a solid picture of applicantsâ€™ credit activity, debt levels, and repayment behavior, all of which are key for understanding default risk.

#### Aggregating Credit Bureau Data by Applicant
```{r}

library(data.table)

# Convert to data.table
bureau_dt <- as.data.table(bureau)

# Aggregation (much faster than dplyr)
bureau_agg <- bureau_dt[, .(
  NUM_BUREAU_RECORDS = .N,
  TOTAL_CREDIT_SUM = sum(AMT_CREDIT_SUM, na.rm = TRUE),
  MEAN_CREDIT_SUM = mean(AMT_CREDIT_SUM, na.rm = TRUE),
  MAX_OVERDUE = max(AMT_CREDIT_SUM_OVERDUE, na.rm = TRUE),
  NUM_ACTIVE = sum(CREDIT_ACTIVE == "Active", na.rm = TRUE),
  NUM_CLOSED = sum(CREDIT_ACTIVE == "Closed", na.rm = TRUE),
  AVG_DAYS_CREDIT = mean(DAYS_CREDIT, na.rm = TRUE)
), by = SK_ID_CURR]

```


#### Joining Bureau Data with Clean Training Dataset

We merged the aggregated credit bureau data with the cleaned application_train.csv dataset using SK_ID_CURR as the key. After joining, we calculated the correlation between the new bureau-level features and the target variable (loan default status). This helps us identify which external credit history metrics may have predictive value for default risk.

```{r}

# convert to data.table 
homecredit_dt <- as.data.table(HomeCredit_imputed)

# Join the datasets
HomeCredit_merged <- merge(
  homecredit_dt, bureau_agg,
  by = "SK_ID_CURR", all.x = TRUE
)

# Make sure TARGET is numeric (0/1)
HomeCredit_merged[, TARGET := as.numeric(as.character(TARGET))]

# Select only new bureau vars
bureau_vars <- c("NUM_BUREAU_RECORDS", "TOTAL_CREDIT_SUM", "MEAN_CREDIT_SUM",
                 "MAX_OVERDUE", "NUM_ACTIVE", "NUM_CLOSED", "AVG_DAYS_CREDIT")

# Compute correlations
corrs <- sapply(HomeCredit_merged[, ..bureau_vars], function(x) {
  cor(HomeCredit_merged$TARGET, x, use = "complete.obs")
})

# Create a tidy table
cor_table <- data.frame(
  Variable = bureau_vars,
  Correlation = round(corrs, 3)
)

print(cor_table)


```
 Most of the variables had very weak correlations, meaning they donâ€™t show a strong direct relationship with default risk. However, a few stood out slightly. For example, the number of active loans (NUM_ACTIVE) and the average number of days since the last credit (AVG_DAYS_CREDIT) showed small positive correlations, suggesting that clients with more active credit or older credit records might be a bit more likely to default.

```{r, warning=FALSE, message=FALSE}
#Max Overdue Amount by Default Status
ggplot(HomeCredit_merged[!is.na(MAX_OVERDUE)], aes(x = factor(TARGET), y = MAX_OVERDUE, fill = factor(TARGET))) +
  geom_boxplot(alpha = 0.7, outlier.size = 0.8) +
  scale_y_log10(labels = scales::comma_format()) +
  scale_fill_manual(values = c("0" = "#f3e6f8", "1" = "#f095f5")) +
  labs(
    title = "Max Overdue Amount by Default Status",
    x = "TARGET",
    y = "MAX_OVERDUE"
  ) +
  theme_minimal()

```
We used a log scale to visualize the maximum overdue amount from credit bureau records (MAX_OVERDUE) by default status. This helps handle the extreme outliers and makes the comparison easier to see. The boxplot shows that clients who defaulted (TARGET = 1) generally had higher overdue amounts than those who didnâ€™t default (TARGET = 0).


# Key Takeaways

## Summary of Findings



### Missing Data

- 37 columns had more than 50% missing values, mostly related to apartment or property characteristics.
- These variables were dropped because of high missingness and low predictive value.
- An exception was `EXT_SOURCE_1`, retained despite ~56% missing due to its strong predictive value.

**Action Taken**:
- Dropped 37 variables with excessive miss data.
- Imputed missing values using:
    - Median for numeric variables  
    - Mode for categorical variables
    - Zero for EXT_SOURCE variables, to represent no credit history


### Extreme Outliers

- Many numeric variables had unrealistic or skewed values.
- Most notable outliers included `DAYS_EMPLOYED = 365243` and income over 100 million.
- Key variables like income and children had outliers that were retained and handled using transformation.

**Action Taken**:
- Re-coded `DAYS_EMPLOYED = 365243` as missing.
- Applied `log()` transformation for `AMT_INCOME_TOTAL` and similar skewed variables.
- Did not drop rows for outliers but used domain-informed thresholds.


### Class Imbalance

- Only 8% of clients defaulted (`TARGET = 1`), meaning the dataset is heavily imbalanced.

**Action Taken**:
- Acknowledged imbalance and planned to use AUC and alternative metrics in modeling.


### Strong Predictors Identified

- External credit scores: `EXT_SOURCE_1`, `EXT_SOURCE_2`, `EXT_SOURCE_3`
- Age: `DAYS_BIRTH`
- Employment history: `DAYS_EMPLOYED`
- Loan structure: `AMT_CREDIT`, `AMT_ANNUITY`, `NAME_CONTRACT_TYPE`


### Correlation Insights

- Highest correlation with TARGET was `DAYS_BIRTH` (0.078), confirming that younger clients are slightly riskier.
- Other variables like region, rating and phone number changes had weak but notable associations.


### Categorical Variables

- Most clients were women, had secondary education, and were married.
- Default rates were relatively stable across these categories.


### Transactional Data (bureau.csv)
- Over 1.7 million credit records aggregated per client.
- Joined to main dataset by using `SK_ID_CURR`.
- Aggregated features like `NUM_ACTIVE`, `AVG_DAYS_CREDIT`, and `MAX_OVERDUE` showed weak but useful signal.

**Action Taken**:
- Aggregate features from `bureau.csv` , like number of activeand closed accounts, total and average credit amounts and  maximum overdue amounts.
- Integrated these features into the main dataset to enrich applicant profiles.
- Performed preliminary statistical tests and correlation analysis with `TARGET` to assess their predictive value before modeling.


### Final Data Cleaning Summary

| Processing Step                                 | Count   |
|--------------------------------------------------|---------|
| Original dataset size                            | 307,511 |
| Columns removed (missingness > 50%)              | 37      |
| Columns retained after filtering                 | 85      |
| Rows flagged as extreme outliers (not dropped)   | 55,376  |
| Missing values remaining after imputation        | 0       |



## Modeling Readiness

- Dataset is cleaned, imputed, and prepped for modeling.
- All columns are encoded properly and no missing values remain.


## Future Steps

### Feature Engineering

- Build ratios like `AMT_CREDIT / AMT_INCOME_TOTAL`
- Normalize `DAYS_BIRTH` and `DAYS_EMPLOYED` relative to age or working-age expectancy


### Imputation Enhancements

- Try KNN  for numeric imputation to preserve data structure
- Consider domain-specific logic for EXT_SOURCE and bureau-level gaps

### Address Class Imbalance

- Use `class_weight = "balanced"` for logistic regression

### Modeling Approaches

- Test multiple algorithms:
    - Logistic Regression (baseline)
    - Random Forest
    - XGBoost
    - Neural Networks
- Evaluate using AUC, Precision-Recall, and Confusion Matrix

## Transition to Predictive Modeling

This refined dataset is now ready to move into predictive modeling. The preprocessing steps takensuch as removing high missingness and redundant features, imputing remaining gaps, and engineering new variables from `bureau.csv` are intended to enhance model performance, interpretability, and stability.

However, the integration of transactional data from `bureau.csv` introduced additional missing values, especially for clients with no credit bureau records.

### Final Recommendations

To ensure strong modeling results moving forward, we recommend the following strategies:

#### Handle Missing Values from `bureau.csv`

- Many applicants have no credit history in the bureau file, resulting in `NA` values in the joined aggregates.
- Treat these `NA`s explicitly by:
  - Imputing with 0 -->if the absence implies "no activity"
- This avoids biased estimates or row drops in models that cannot handle missing values.
