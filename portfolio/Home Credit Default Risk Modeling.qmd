---
title: "Home Credit Default Risk Modeling "
author: "Group 4 - Corrin Childs, Gaby Rodriguez, Joel Jorgensen, Josh Mcalister"
date: today
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false

---

# Load Data and Setup

```{r library}
pacman::p_load(tidyverse, caret, janitor, skimr, recipes, themis, rlang, tidymodels, here)
```

Data is all loaded in. 
```{r load data application train}

dir<- getwd()
setwd(dir)

#load file - assumes you have copy of file in working directory
hc_train <-read.csv("application_train.csv", stringsAsFactors = TRUE)


#standardize variable names
hc_train <- hc_train |>
  rename_with(tolower)

#convert target and select numeric indicators to factors
hc_train <- hc_train |>
  mutate(
    target = as.factor(target),
    across(starts_with("flag"),as.factor),
    across(starts_with("reg_"),as.factor),
    across(starts_with("live_"),as.factor)
    )
```

```{r load data bureau}
#load file - assumes you have copy of file in working directory
bureau_data <- read.csv("bureau.csv", stringsAsFactors = TRUE)

#standardize variable names
bureau_data <- bureau_data |>
  rename_with(tolower)

```

```{r load data installment}
#load file - assumes you have copy of file in working directory
installment_data <- read.csv("installments_payments.csv", stringsAsFactors = TRUE)

#standardize variable names
installment_data <- installment_data |>
  rename_with(tolower)

```

```{r load credit card data}
#load file - assumes you have copy of file in working directory
credit_card_data <- read.csv("credit_card_balance.csv", stringsAsFactors = TRUE)

#filter to only include selected columns
credit_card_data <- credit_card_data |>
  select(
    SK_ID_CURR,
    MONTHS_BALANCE,
    AMT_BALANCE,
    AMT_CREDIT_LIMIT_ACTUAL,
    AMT_INST_MIN_REGULARITY,
    AMT_PAYMENT_CURRENT,
    AMT_PAYMENT_TOTAL_CURRENT,
    SK_DPD
  )
```

```{r load pos bal data}
#load file - assumes you have copy of file in working directory
pos_cash_data <-read.csv("POS_CASH_balance.csv", stringsAsFactors = TRUE)

#filter to only include selected columns
pos_cash_data <- pos_cash_data |>
  select(SK_ID_CURR,
         MONTHS_BALANCE,
         CNT_INSTALMENT,
         CNT_INSTALMENT_FUTURE,
         SK_DPD
  )
```


# Additional Data Source Preperation
Calculated columns are created for the additional data sources. Then for each an aggregation is performed to try and capture any relevant signal while getting it so their is one row for each client. 

## Bureau Data
```{r created aggregated bureau data}
#creates new data frame of aggregated bureau data
bureau_agg <- bureau_data |>
    group_by(sk_id_curr) |>
    summarise(
      bur_n_loans = n(),
      #status counts
      bur_n_active       = sum(credit_active == "Active", na.rm = TRUE),
      bur_n_closed       = sum(credit_active == "Closed", na.rm = TRUE),
      #min/max days since credit application
      bur_min_days_credit = suppressWarnings(min(days_credit, na.rm = TRUE)),
      bur_max_days_credit = suppressWarnings(max(days_credit, na.rm = TRUE)),
      #amount aggregates
      bur_sum_credit_sum     = sum(amt_credit_sum, na.rm = TRUE),
      bur_sum_credit_debt    = sum(amt_credit_sum_debt, na.rm = TRUE),
      bur_sum_credit_overdue = sum(amt_credit_sum_overdue, na.rm = TRUE),
      bur_sum_credit_limit   = sum(amt_credit_sum_limit, na.rm = TRUE),
      #max overdue
      bur_max_amt_credit_max_overdue = suppressWarnings(max(amt_credit_max_overdue, na.rm = TRUE)),
      #credit type counts
      bur_n_mortgage         = sum(credit_type == "Mortgage", na.rm = TRUE),
      bur_n_auto             = sum(credit_type == "Car loan", na.rm = TRUE),
      bur_n_consumer_credit  = sum(credit_type == "Consumer credit", na.rm = TRUE),
      bur_n_credit_card      = sum(credit_type == "Credit card", na.rm = TRUE),
      .groups = "drop"
    )
#removes infinity values created from min/max functions and changes to 0, INF and - INF occur if min/max is ran on NA values
bureau_agg <- bureau_agg |> mutate(across(where(is.numeric), ~ ifelse(is.infinite(.x), NA_real_, .x)))
#replace any remaining NA from aggregation with 0
bureau_agg <- bureau_agg |>
  mutate(across(where(is.numeric), ~ replace_na(.x, 0)))

# check for any remaining NAs - none
#bureau_agg |>
  #filter(if_any(everything(), is.na))
```

## Installment Data

```{r installment data}
#take differences prior to aggregation
installment_data <- installment_data |> 
  mutate(day_dif = days_instalment - days_entry_payment # difference between loan is due and when it was paid
        ,amt_dif = amt_instalment - amt_payment # difference between amount of payment due and what was paid
  )

# 2905 rows have NAs, which shows they haven't actually made that payment
colSums(is.na(installment_data[, c("day_dif", "amt_dif")]))
summary(installment_data[, c("day_dif", "amt_dif")])

# view head of data
installment_data |>
  filter(is.na(day_dif) | is.na(amt_dif)) |> head(10)


# remove instalment amounts = to 0
installment_data <- installment_data |>
  filter(amt_instalment != 0)

```

```{r installment data continued}
#below creates calculated fields before aggregation is performed

# set day dif to how long hasn't been paid if no payment
installment_data <- installment_data |> mutate(day_dif = ifelse(is.na(day_dif),days_instalment,day_dif))
# set amt dif to how much is unpaid if no payment
installment_data <- installment_data |> mutate(amt_dif = ifelse(is.na(amt_dif),amt_instalment,amt_dif))
# get ratio of payment difference compared to payment due, flag if paid on time, flag if payment missed
installment_data <- installment_data |> mutate(paid_ratio = amt_dif / amt_instalment,
                                               paid_late = ifelse(day_dif <= 0, 1,0),
                                               missed_payment = ifelse(paid_ratio == 1, 1, 0))
#filter to only have needed columns
installment_data <- installment_data |> select(sk_id_curr, day_dif, amt_dif, paid_ratio, paid_late, missed_payment)
# positive day dif is early payment, 0 is ontime, negative is late
```

```{r installment data aggregation}
#aggregate installment data
installment_agg <- installment_data |>
  group_by(sk_id_curr) |>
  summarise(
    in_n_installments = n(),
    in_avg_day_dif = mean(day_dif, na.rm = TRUE),
    in_min_day_dif = min(day_dif, na.rm = TRUE),
    in_max_day_dif = max(day_dif, na.rm = TRUE),
    in_avg_amt_dif = mean(amt_dif, na.rm = TRUE),
    in_total_amt_dif = sum(amt_dif, na.rm = TRUE),
    in_avg_paid_ratio = mean(paid_ratio, na.rm = TRUE),
    in_share_paid_late = mean(paid_late, na.rm = TRUE),
    in_share_missed_pmt = mean(missed_payment, na.rm = TRUE),
    in_n_paid_late = sum(paid_late, na.rm = TRUE),
    in_n_missed_pmt = sum(missed_payment, na.rm = TRUE),
    .groups = "drop"
  )
#removes infinity values
installment_agg <- installment_agg |> mutate(across(where(is.numeric), ~ ifelse(is.infinite(.x), NA_real_, .x)))
#replace any remaining NA from aggregation with 0
installment_agg <- installment_agg |>
  mutate(across(where(is.numeric), ~ replace_na(.x, 0)))

```

## Credit Card Data

```{r credit card data}
#cleanup NAs
credit_card_data <- credit_card_data |> 
  mutate(AMT_PAYMENT_CURRENT = ifelse(is.na(AMT_PAYMENT_CURRENT),0,AMT_PAYMENT_CURRENT),
         AMT_INST_MIN_REGULARITY =ifelse(is.na(AMT_INST_MIN_REGULARITY),0,AMT_INST_MIN_REGULARITY))

# safe_division funciton to avoid Inf/Na when denominator is 0 or NA
safe_div <- function(num, den) ifelse(is.na(den) | den <= 0, 0, num / den)

#create additional columns
credit_card_data <- credit_card_data |>
  mutate(
    util = safe_div(AMT_BALANCE, AMT_CREDIT_LIMIT_ACTUAL),
    pay_vs_min = safe_div(AMT_PAYMENT_CURRENT, AMT_INST_MIN_REGULARITY),
    pay_to_balance = safe_div(AMT_PAYMENT_TOTAL_CURRENT, AMT_BALANCE), 
    is_delinquent = SK_DPD > 0,                                           
    paid_min = AMT_INST_MIN_REGULARITY > 0 & AMT_PAYMENT_CURRENT >= AMT_INST_MIN_REGULARITY,
    paid_full = AMT_BALANCE > 0 & AMT_PAYMENT_TOTAL_CURRENT >= AMT_BALANCE,      
  )
# check for any NAs
#credit_card_data |>
  #filter(if_any(everything(), is.na))
```

```{r credit card aggregation}
# aggregated data set
credit_card_agg <- credit_card_data |>
  group_by(SK_ID_CURR) |>
  summarise(
    # time covered
    cc_n_months            = n(),
    cc_last_month          = max(MONTHS_BALANCE, na.rm = TRUE),
    cc_span_months         = max(MONTHS_BALANCE, na.rm = TRUE) - 
                          min(MONTHS_BALANCE, na.rm = TRUE),
    # balances, limits, utilization
    cc_avg_balance         = mean(AMT_BALANCE, na.rm = TRUE),
    cc_max_balance         = max(AMT_BALANCE, na.rm = TRUE),
    cc_avg_limit           = mean(AMT_CREDIT_LIMIT_ACTUAL, na.rm = TRUE),
    cc_util_mean           = mean(util, na.rm = TRUE),
    cc_util_max            = max(util, na.rm = TRUE),
    # payments vs due
    cc_pay_vs_min_mean     = mean(pay_vs_min, na.rm = TRUE),
    cc_pay_to_balance_mean = mean(pay_to_balance, na.rm = TRUE),
    # delinquency/behavior
    cc_any_dpd             = any(is_delinquent, na.rm = TRUE),
    cc_share_dpd           = mean(is_delinquent, na.rm = TRUE),
    cc_max_dpd             = max(SK_DPD, na.rm = TRUE),
    cc_share_paid_min      = mean(paid_min, na.rm = TRUE),
    cc_share_paid_full     = mean(paid_full, na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(
    across(where(is.numeric), ~ ifelse(is.infinite(.x), NA_real_, .x))
  )
#forgot to turn to lowercase initially so tidy up
credit_card_agg <- credit_card_agg |>
  rename_with(tolower)
#check for any NA
#credit_card_agg |>
  #filter(if_any(everything(), is.na))
```
## POS Cash Balance

```{r pos cash data}
#cleanup NAs
pos_cash_data <- pos_cash_data |> mutate(CNT_INSTALMENT = ifelse(is.na(CNT_INSTALMENT),0,CNT_INSTALMENT),
                CNT_INSTALMENT_FUTURE = ifelse(is.na(CNT_INSTALMENT_FUTURE),0,CNT_INSTALMENT_FUTURE)
)

# create additional columns
pos_cash_data <- pos_cash_data |>
  mutate(
    # paid and remaining ratios for installments
    share_paid = safe_div(CNT_INSTALMENT - CNT_INSTALMENT_FUTURE, CNT_INSTALMENT),
    share_remaining = safe_div(CNT_INSTALMENT_FUTURE, CNT_INSTALMENT),
    # deliquencies 
    is_dpd = SK_DPD > 0
  )



```


```{r pos cash data agg}
#create aggregation
pos_cash_agg <- pos_cash_data |>
  group_by(SK_ID_CURR) |>
  summarise(
    # duration of months
    pos_n_months        = n(),
    pos_last_month      = max(MONTHS_BALANCE, na.rm = TRUE),
    pos_span_months     = pos_last_month - min(MONTHS_BALANCE, na.rm = TRUE),
    # term and future ratios
    pos_term_median     = median(CNT_INSTALMENT, na.rm = TRUE),
    pos_term_max        = max(CNT_INSTALMENT, na.rm = TRUE),
    pos_future_median   = median(CNT_INSTALMENT_FUTURE, na.rm = TRUE),
    pos_future_min      = min(CNT_INSTALMENT_FUTURE, na.rm = TRUE),
    # repayment progress ratios
    pos_share_paid_mean = mean(share_paid, na.rm = TRUE),
    pos_share_rem_mean  = mean(share_remaining, na.rm = TRUE),
    # delinquency
    pos_any_dpd         = any(is_dpd, na.rm = TRUE),
    pos_share_dpd       = mean(is_dpd, na.rm = TRUE),
    pos_max_dpd         = max(SK_DPD, na.rm = TRUE),
    .groups = "drop"
  ) |>
  # replace inf
  mutate(across(where(is.numeric), ~ ifelse(is.infinite(.x), NA_real_, .x)))

#check for any NA
#pos_cash_agg |>
  #filter(if_any(everything(), is.na))
#forgot to turn to lowercase initially so tidy up
pos_cash_agg <- pos_cash_agg |>
  rename_with(tolower)

```




# Application Data Preperation


## Feature Selection Missing Data and NAs
Variables where greater than 50% of values were missing were dropped. The one exception was ext_source_1. A good portion of those dropped were related to those living in apartments and the characteristics of their housing. 

83 feature variables are retained. 
```{r drop missing data application}
# % missing per variable (full dataset)
missing_perc <- colMeans(is.na(hc_train))

# Keep rule: ≤ 50% missing
cols_to_keep <- names(missing_perc[missing_perc <= 0.50])

# Exception: always keep EXT_SOURCE_1
cols_final <- unique(c(cols_to_keep, "ext_source_1"))

# Filter dataset
hc_train <- hc_train |>
  dplyr::select(all_of(cols_final))

```

## Outliers
One outlier value has been identified and that is the highest income earner listed, this value is removed. 
Also the days employed uses an extreme value for place holding when a person is not employed, so this value is changed from 365,243 to 0. 

```{r remove outliers}
#filter to remove incomes more than 100million
hc_train |> filter(amt_income_total <= 100000000) -> hc_train
#mutate days employed to 0 for positive values
hc_train |> mutate(days_employed = ifelse(days_employed > 0,0,days_employed)) -> hc_train
```



## Joining Data
All four aggregated sets of data are left joined onto application training data set.
```{r merge data}
#join data
hc_merged <- hc_train |> left_join(bureau_agg, by = "sk_id_curr")
hc_merged <- hc_merged |> left_join(installment_agg, by = "sk_id_curr")
hc_merged <- hc_merged |> left_join(credit_card_agg, by = "sk_id_curr")
hc_merged <- hc_merged |> left_join(pos_cash_agg, by = "sk_id_curr")

```


# Training and Test Partions
Splitting into a 80/20 train/test partition split. 

```{r Q3}
set.seed(61)
#create data partition index
index <- createDataPartition(y = hc_merged$target, p = 0.8, list = FALSE)
#create split data frames
train_hc_merged <- hc_merged[index,]
holdout_hc_merged <- hc_merged[-index,] # do not use at all - only for very last run once model picked

```


## Missing Data Imputation and Log Scaling
Imputation of missing values and NAs is done last after data is all joined and split to avoid leakage. Income is log transformed due to it's heavy right skew. Modal values are imputed for categorical, median for numeric. Finally numeric data is scaled to account for certain models being sensitive to magnitude differences of variables. 

A data cleaning recipe is created that can be used on all test and training data. 
For training data an additional recipe is used that creates oversampled data set for use.
```{r recipe creation}
#recipe for data cleaning - to be used on all future data sets
rec <- recipe(target ~ ., data = train_hc_merged) |>
  update_role(sk_id_curr, new_role = "id") |> # set so ID doesn't get touched
  step_zv(all_predictors()) |> #remove zero variance predictors
  step_indicate_na(all_predictors()) |> #flag variable value that was NA, possible signal
  step_impute_median(all_numeric_predictors()) |> #impute medians on NA
  step_impute_mode(all_nominal_predictors()) |> #impute modal for non numeric
  step_mutate(amt_income_total = log1p(amt_income_total)) |> # log scale income - 1p for 0 vals
  step_normalize(all_numeric_predictors(), -starts_with("na_ind_")) |> #scales all numeric - svm/logistic sensitive - doesn't scale flagged NA created values
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |> #one hot encodes all categorical
  step_zv(all_predictors()) #remove zero variance predictors after dummys created

```

Oversampling techniques evaluated
SMOTE - synthetic minority oversampling - creates synthetic nearest neighbor points (two big weaknesses of high n dimensions and categorical data) - for this reason not used

Upsample - randomly oversampling with replacement, most simple and straight forward

ROSE - random oversampling examples - creates synthetic data points using kernel density estimation (small item is it can create nonsensical examples, no other cons except computationally intensive and sensitive to tuning) - for this reason not used but could be revisited


Oversample ratio is used as starting point of .5 (2 to 1 for majority vs minority). This can be tweaked as needed later
```{r over sample recipe}
# baseline recipe for no over sampling
rec_base  <- rec
#recipe for oversampling - adds oversampling at the end of data prep
rec_over_sample <- rec |> step_upsample(target, over_ratio = 0.5, skip = TRUE)

```



```{r apply recipe to data}
#train recipe on training data split
prep_base <- prep(rec_base, training = train_hc_merged, verbose = TRUE)
prep_over <- prep(rec_over_sample, training = train_hc_merged, verbose = TRUE)
#create data frames
x_train_base <- juice(prep_base)
x_train_over_sample <- juice(prep_over)

```

```{r over sample check}
#quick sanity check for oversampling class balances

#x_train_base |> count(target)
#round(19860 / 226149,2)

#x_train_over_sample |> count(target)
#round(113074 / 226149,2)
```


#Baseline Logistic Regression 
## Benchmark using a Model

We start with a null logistic regression that includes only an intercept (no predictors). This model predicts everyone’s default probability as the overall average in the training data. It serves as our baseline to compare more complex models. If future models don’t improve accuracy or AUC beyond this benchmark, it means the added predictors don’t add real value.
 
```{r, echo = FALSE, warning = FALSE}
# Logistic Regression Modeling

# Benchmark Model 

library(pROC)
library(yardstick)
library(dplyr)
library(tibble)

#Benchmark Model-Logistic Regression
#This is the baseline Benchmark
null_model <- glm(target ~ 1, data = x_train_base, family = "binomial")
pred_null <- predict(null_model, type = "response")


#Creates a table with the actual labels, predicted probabilities and predicted classes and predicted classes (.pred_class, 1 if ≥ 0.5, else 0).
null_tbl <- tibble(
  truth = factor(x_train_base$target, levels = c(0, 1)),
  .pred = pred_null,
  .pred_class = factor(ifelse(pred_null >= 0.5, 1, 0), levels = c(0, 1))
)

#Calculate Accuracy and ROC AUC
benchmark_results <- metric_set(accuracy, roc_auc)(null_tbl, truth = truth, estimate = .pred_class, .pred)
benchmark_results$model <- "Null (Intercept Only)"


# Logistic Regression Model (Selected Predictors)
#Predicts the probability that each applicant defaults
log_model <- glm(
  target ~ amt_income_total + days_employed +
    ext_source_1 + ext_source_2 + ext_source_3 +
    bur_n_loans + bur_sum_credit_sum +
    in_share_paid_late + in_n_missed_pmt +
    cc_util_mean + cc_share_dpd + cc_max_dpd +
    pos_share_paid_mean + pos_share_dpd + pos_max_dpd,
  data = x_train_base,
  family = binomial
)

pred_log <- predict(log_model, newdata = x_train_base, type = "response")

log_tbl <- tibble(
  truth = factor(x_train_base$target, levels = c(0, 1)),
  .pred = pred_log,
  .pred_class = factor(ifelse(pred_log >= 0.5, 1, 0), levels = c(0, 1))
)

log_results <- metric_set(accuracy, roc_auc)(log_tbl, truth = truth, estimate = .pred_class, .pred)
log_results$model <- "Logistic Regression (Selected Vars)"


# Logistic Regression Model (Oversampled Data)
#Same model formula, but now trained on the oversampled data (x_train_over_sample).To handle class imbalance
log_over <- glm(
  target ~ amt_income_total + days_employed +
    ext_source_1 + ext_source_2 + ext_source_3 +
    bur_n_loans + bur_sum_credit_sum +
    in_share_paid_late + in_n_missed_pmt +
    cc_util_mean + cc_share_dpd + cc_max_dpd +
    pos_share_paid_mean + pos_share_dpd + pos_max_dpd,
  data = x_train_over_sample,
  family = binomial
)

pred_over <- predict(log_over, newdata = x_train_over_sample, type = "response")

over_tbl <- tibble(
  truth = factor(x_train_over_sample$target, levels = c(0, 1)),
  .pred = pred_over,
  .pred_class = factor(ifelse(pred_over >= 0.5, 1, 0), levels = c(0, 1))
)

over_results <- metric_set(accuracy, roc_auc)(over_tbl, truth = truth, estimate = .pred_class, .pred)
over_results$model <- "Logistic Regression (Oversampled)"


# Summary Results
final_results <- bind_rows(benchmark_results, log_results, over_results) |>
  select(model, .metric, .estimate) |>
  tidyr::pivot_wider(names_from = .metric, values_from = .estimate) |>
  arrange(desc(roc_auc))

print(final_results)

```






