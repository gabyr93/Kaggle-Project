---
title: "Home Credit Default Risk Modeling "
author: "Gaby Rodriguez"
date: today
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false

---

# Introduction
Home Credit is a multinational provider focused on making financial services more accessible to people without traditional credit histories, often referred to as the unbanked. This group represents missed revenue for the company and a poor experience for prospective borrowers. The company’s goal is to provide fair opportunities to individuals who are often excluded from borrowing. A variety of data sources are available to assess creditworthiness, enabling Home Credit to extend loans to those traditionally considered unbanked. In this project, we use the available data sources to build predictive models that estimate the probability of default. These insights will help Home Credit make smarter lending decisions, reduce financial risk, and give qualified clients a better chance to succeed.

# Business Problem
Home Credit is a company that focuses on making financial services more accessible for people who don’t have traditional credit histories, often referred to as the unbanked. Their goal is to give fair opportunities to those who are often excluded from borrowing. One of their main challenges is figuring out which clients are likely to repay their loans. In this project, we’ll use the data to build predictive models that estimate the chance of default. These insights will help Home Credit make smarter lending decisions, reduce financial risk, and give qualified clients a better chance to succeed.

## Analtyics Problem
Home Credit customers include those with and without traditional credit histories. In evaluating which customers should be extended credit, the company needs a predictive model for determining which customers are likely to experience loan repayment difficulties. The company needs a single model that makes use of all the available data for a given customer, to make the most accurate prediction. For different customers, the available data will differ. For all clients we have data provided as part of the application process, though not all data points are available or applicable for each customer. In addition to this we have four other data sets that have potentially relevant details, the presence of this data for a given customer also differs. A single joined data set is compiled with all available data, with this data we trained a model that best makes use of the available information while accounting for the cases where information is lacking for a given customer.  The actual process of data preparation will be handled in a future section. 


# Load Data and Setup

First step is setting up workbook with needed packages, and loading in all of the data sets. 

```{r library}
pacman::p_load(tidyverse, caret, janitor, skimr, recipes, themis, rlang, tidymodels, here, AppliedPredictiveModeling, brulee, torch, pROC,yardstick,tibble, glmnet, randomForest, tidymodels, xgboost, caretEnsemble)
```

```{r load data application train}
#load file - assumes you have copy of file in working directory
hc_train <- read.csv(here::here("data", "application_train.csv"), stringsAsFactors = TRUE)

#standardize variable names
hc_train <- hc_train |>
  rename_with(tolower)

#convert target and select numeric indicators to factors
hc_train <- hc_train |>
  mutate(
    target = as.factor(target),
    across(starts_with("flag"),as.factor),
    across(starts_with("reg_"),as.factor),
    across(starts_with("live_"),as.factor)
    )
```

```{r load data bureau}
#load file - assumes you have copy of file in working directory
bureau_data <- read.csv(here::here("data", "bureau.csv"), stringsAsFactors = TRUE)

#standardize variable names
bureau_data <- bureau_data |>
  rename_with(tolower)

```

```{r load data installment}
#load file - assumes you have copy of file in working directory
installment_data <- read.csv(here::here("data", "installments_payments.csv"), stringsAsFactors = TRUE)

#standardize variable names
installment_data <- installment_data |>
  rename_with(tolower)

```

```{r load credit card data}
#load file - assumes you have copy of file in working directory
credit_card_data <- read.csv(here::here("data","credit_card_balance.csv"), stringsAsFactors = TRUE)

#filter to only include selected columns
credit_card_data <- credit_card_data |>
  select(
    SK_ID_CURR,
    MONTHS_BALANCE,
    AMT_BALANCE,
    AMT_CREDIT_LIMIT_ACTUAL,
    AMT_INST_MIN_REGULARITY,
    AMT_PAYMENT_CURRENT,
    AMT_PAYMENT_TOTAL_CURRENT,
    SK_DPD
  )
```

```{r load pos bal data}
#load file - assumes you have copy of file in working directory
pos_cash_data <- read.csv(here::here("data","POS_CASH_balance.csv"), stringsAsFactors = TRUE)

#filter to only include selected columns
pos_cash_data <- pos_cash_data |>
  select(SK_ID_CURR,
         MONTHS_BALANCE,
         CNT_INSTALMENT,
         CNT_INSTALMENT_FUTURE,
         SK_DPD
  )
```

# Data Cleaning and Preparation
For application data a few initial preparation steps were completed. First, we removed any columns where over half of the data was missing, with one exception of ext_source_1 which ~56% was missing. For the dropped columns with so much missing we would likely be creating more noise by trying to impute values for all the missing data.  Also, the columns removed don’t appear to be that relevant, for example a good portion are characteristics about the building a person lives in. The exception was made for ext_source_1 which is a credit score metric and likely has predictive power. Outliers were evaluated and only one data point needed to be removed, for an individual with an income over 100 million, all other rows were left intact. 


Four additional sources were brought in they are as follows.

- **Credit Bureau - ** All client's previous credits provided by other financial institutions that were reported to Credit Bureau

- **Installment Loan -** Repayment history for the previously disbursed credits in Home Credit related to the loans in our sample.

- **Credit Card -** Monthly balance snapshots of previous credit cards that the applicant has with Home Credit

- **Point of Sales and Cash Loans -** Monthly balance snapshots of previous POS (point of sales) and cash loans that the applicant had with Home Credit.

## Additional Data Source Preperation

For the below four data sets, if data existed for a client it often had several rows, to account for this in each data set select data metrics were aggregated at a customer level. These were various min, max, summed, average and difference values. For each data set quite a few where metrics were compiled for the available data points, so the exact details won’t be discussed. 

### Bureau Data
```{r created aggregated bureau data}
#creates new data frame of aggregated bureau data
bureau_agg <- bureau_data |>
    group_by(sk_id_curr) |>
    summarise(
      bur_n_loans = n(),
      #status counts
      bur_n_active       = sum(credit_active == "Active", na.rm = TRUE),
      bur_n_closed       = sum(credit_active == "Closed", na.rm = TRUE),
      #min/max days since credit application
      bur_min_days_credit = suppressWarnings(min(days_credit, na.rm = TRUE)),
      bur_max_days_credit = suppressWarnings(max(days_credit, na.rm = TRUE)),
      #amount aggregates
      bur_sum_credit_sum     = sum(amt_credit_sum, na.rm = TRUE),
      bur_sum_credit_debt    = sum(amt_credit_sum_debt, na.rm = TRUE),
      bur_sum_credit_overdue = sum(amt_credit_sum_overdue, na.rm = TRUE),
      bur_sum_credit_limit   = sum(amt_credit_sum_limit, na.rm = TRUE),
      #max overdue
      bur_max_amt_credit_max_overdue = suppressWarnings(max(amt_credit_max_overdue, na.rm = TRUE)),
      #credit type counts
      bur_n_mortgage         = sum(credit_type == "Mortgage", na.rm = TRUE),
      bur_n_auto             = sum(credit_type == "Car loan", na.rm = TRUE),
      bur_n_consumer_credit  = sum(credit_type == "Consumer credit", na.rm = TRUE),
      bur_n_credit_card      = sum(credit_type == "Credit card", na.rm = TRUE),
      .groups = "drop"
    )
#removes infinity values created from min/max functions and changes to 0, INF and - INF occur if min/max is ran on NA values
bureau_agg <- bureau_agg |> mutate(across(where(is.numeric), ~ ifelse(is.infinite(.x), NA_real_, .x)))
#replace any remaining NA from aggregation with 0
bureau_agg <- bureau_agg |>
  mutate(across(where(is.numeric), ~ replace_na(.x, 0)))

# check for any remaining NAs - none
#bureau_agg |>
  #filter(if_any(everything(), is.na))
```

### Installment Data

```{r installment data}
#take differences prior to aggregation
installment_data <- installment_data |>
  mutate(
    day_dif = days_instalment - days_entry_payment,  # Difference between due date and payment date
    amt_dif = amt_instalment - amt_payment           # Difference between amount due and amount paid
  )

# 2905 rows have NAs, which shows they haven't actually made that payment
colSums(is.na(installment_data[, c("day_dif", "amt_dif")]))
summary(installment_data[, c("day_dif", "amt_dif")])

# view head of data
installment_data |>
  filter(is.na(day_dif) | is.na(amt_dif)) |> head(10)

# remove instalment amounts = to 0
installment_data <- installment_data |>
  filter(amt_instalment != 0)
```

```{r installment data continued}
#below creates calculated fields before aggregation is performed

# set day dif to how long hasn't been paid if no payment
installment_data <- installment_data |> mutate(day_dif = ifelse(is.na(day_dif),days_instalment,day_dif))
# set amt dif to how much is unpaid if no payment
installment_data <- installment_data |> mutate(amt_dif = ifelse(is.na(amt_dif),amt_instalment,amt_dif))
# get ratio of payment difference compared to payment due, flag if paid on time, flag if payment missed
installment_data <- installment_data |> mutate(paid_ratio = amt_dif / amt_instalment,
                                               paid_late = ifelse(day_dif <= 0, 1,0),
                                               missed_payment = ifelse(paid_ratio == 1, 1, 0))
#filter to only have needed columns
installment_data <- installment_data |> select(sk_id_curr, day_dif, amt_dif, paid_ratio, paid_late, missed_payment)
# positive day dif is early payment, 0 is ontime, negative is late
```

```{r installment data aggregation}
#aggregate installment data
installment_agg <- installment_data |>
  group_by(sk_id_curr) |>
  summarise(
    in_n_installments = n(),
    in_avg_day_dif = mean(day_dif, na.rm = TRUE),
    in_min_day_dif = min(day_dif, na.rm = TRUE),
    in_max_day_dif = max(day_dif, na.rm = TRUE),
    in_avg_amt_dif = mean(amt_dif, na.rm = TRUE),
    in_total_amt_dif = sum(amt_dif, na.rm = TRUE),
    in_avg_paid_ratio = mean(paid_ratio, na.rm = TRUE),
    in_share_paid_late = mean(paid_late, na.rm = TRUE),
    in_share_missed_pmt = mean(missed_payment, na.rm = TRUE),
    in_n_paid_late = sum(paid_late, na.rm = TRUE),
    in_n_missed_pmt = sum(missed_payment, na.rm = TRUE),
    .groups = "drop"
  )
#removes infinity values
installment_agg <- installment_agg |> mutate(across(where(is.numeric), ~ ifelse(is.infinite(.x), NA_real_, .x)))
#replace any remaining NA from aggregation with 0
installment_agg <- installment_agg |>
  mutate(across(where(is.numeric), ~ replace_na(.x, 0)))

```

### Credit Card Data

```{r credit card data}
#cleanup NAs
credit_card_data <- credit_card_data |> 
  mutate(AMT_PAYMENT_CURRENT = ifelse(is.na(AMT_PAYMENT_CURRENT),0,AMT_PAYMENT_CURRENT),
         AMT_INST_MIN_REGULARITY =ifelse(is.na(AMT_INST_MIN_REGULARITY),0,AMT_INST_MIN_REGULARITY))

# safe_division funciton to avoid Inf/Na when denominator is 0 or NA
safe_div <- function(num, den) ifelse(is.na(den) | den <= 0, 0, num / den)

#create additional columns
credit_card_data <- credit_card_data |>
  mutate(
    util = safe_div(AMT_BALANCE, AMT_CREDIT_LIMIT_ACTUAL),
    pay_vs_min = safe_div(AMT_PAYMENT_CURRENT, AMT_INST_MIN_REGULARITY),
    pay_to_balance = safe_div(AMT_PAYMENT_TOTAL_CURRENT, AMT_BALANCE), 
    is_delinquent = SK_DPD > 0,                                           
    paid_min = AMT_INST_MIN_REGULARITY > 0 & AMT_PAYMENT_CURRENT >= AMT_INST_MIN_REGULARITY,
    paid_full = AMT_BALANCE > 0 & AMT_PAYMENT_TOTAL_CURRENT >= AMT_BALANCE,      
  )
# check for any NAs
#credit_card_data |>
  #filter(if_any(everything(), is.na))
```

```{r credit card aggregation}
# aggregated data set
credit_card_agg <- credit_card_data |>
  group_by(SK_ID_CURR) |>
  summarise(
    # time covered
    cc_n_months            = n(),
    cc_last_month          = max(MONTHS_BALANCE, na.rm = TRUE),
    cc_span_months         = max(MONTHS_BALANCE, na.rm = TRUE) - 
                          min(MONTHS_BALANCE, na.rm = TRUE),
    # balances, limits, utilization
    cc_avg_balance         = mean(AMT_BALANCE, na.rm = TRUE),
    cc_max_balance         = max(AMT_BALANCE, na.rm = TRUE),
    cc_avg_limit           = mean(AMT_CREDIT_LIMIT_ACTUAL, na.rm = TRUE),
    cc_util_mean           = mean(util, na.rm = TRUE),
    cc_util_max            = max(util, na.rm = TRUE),
    # payments vs due
    cc_pay_vs_min_mean     = mean(pay_vs_min, na.rm = TRUE),
    cc_pay_to_balance_mean = mean(pay_to_balance, na.rm = TRUE),
    # delinquency/behavior
    cc_any_dpd             = any(is_delinquent, na.rm = TRUE),
    cc_share_dpd           = mean(is_delinquent, na.rm = TRUE),
    cc_max_dpd             = max(SK_DPD, na.rm = TRUE),
    cc_share_paid_min      = mean(paid_min, na.rm = TRUE),
    cc_share_paid_full     = mean(paid_full, na.rm = TRUE),
    .groups = "drop"
  ) |>
  mutate(
    across(where(is.numeric), ~ ifelse(is.infinite(.x), NA_real_, .x))
  )
#forgot to turn to lowercase initially so tidy up
credit_card_agg <- credit_card_agg |>
  rename_with(tolower)
#check for any NA
#credit_card_agg |>
  #filter(if_any(everything(), is.na))
```
### POS Cash Balance

```{r pos cash data}
#cleanup NAs
pos_cash_data <- pos_cash_data |> mutate(CNT_INSTALMENT = ifelse(is.na(CNT_INSTALMENT),0,CNT_INSTALMENT),
                CNT_INSTALMENT_FUTURE = ifelse(is.na(CNT_INSTALMENT_FUTURE),0,CNT_INSTALMENT_FUTURE)
)

# create additional columns
pos_cash_data <- pos_cash_data |>
  mutate(
    # paid and remaining ratios for installments
    share_paid = safe_div(CNT_INSTALMENT - CNT_INSTALMENT_FUTURE, CNT_INSTALMENT),
    share_remaining = safe_div(CNT_INSTALMENT_FUTURE, CNT_INSTALMENT),
    # deliquencies 
    is_dpd = SK_DPD > 0
  )



```


```{r pos cash data agg}
#create aggregation
pos_cash_agg <- pos_cash_data |>
  group_by(SK_ID_CURR) |>
  summarise(
    # duration of months
    pos_n_months        = n(),
    pos_last_month      = max(MONTHS_BALANCE, na.rm = TRUE),
    pos_span_months     = pos_last_month - min(MONTHS_BALANCE, na.rm = TRUE),
    # term and future ratios
    pos_term_median     = median(CNT_INSTALMENT, na.rm = TRUE),
    pos_term_max        = max(CNT_INSTALMENT, na.rm = TRUE),
    pos_future_median   = median(CNT_INSTALMENT_FUTURE, na.rm = TRUE),
    pos_future_min      = min(CNT_INSTALMENT_FUTURE, na.rm = TRUE),
    # repayment progress ratios
    pos_share_paid_mean = mean(share_paid, na.rm = TRUE),
    pos_share_rem_mean  = mean(share_remaining, na.rm = TRUE),
    # delinquency
    pos_any_dpd         = any(is_dpd, na.rm = TRUE),
    pos_share_dpd       = mean(is_dpd, na.rm = TRUE),
    pos_max_dpd         = max(SK_DPD, na.rm = TRUE),
    .groups = "drop"
  ) |>
  # replace inf
  mutate(across(where(is.numeric), ~ ifelse(is.infinite(.x), NA_real_, .x)))

#check for any NA
#pos_cash_agg |>
  #filter(if_any(everything(), is.na))
#forgot to turn to lowercase initially so tidy up
pos_cash_agg <- pos_cash_agg |>
  rename_with(tolower)

```


## Application Data Preperation


### Feature Selection Missing Data and NAs
Variables where greater than 50% of values were missing were dropped. The one exception was ext_source_1. A good portion of those dropped were related to those living in apartments and the characteristics of their housing. 

83 feature variables are retained. 
```{r drop missing data application}
# % missing per variable (full dataset)
missing_perc <- colMeans(is.na(hc_train))

# Keep rule: ≤ 50% missing
cols_to_keep <- names(missing_perc[missing_perc <= 0.50])

# Exception: always keep EXT_SOURCE_1
cols_final <- unique(c(cols_to_keep, "ext_source_1"))

# Filter dataset
hc_train <- hc_train |>
  dplyr::select(all_of(cols_final))

```

### Outliers
One outlier value has been identified and that is the highest income earner listed, this value is removed. Also the days employed uses an extreme value for place holding when a person is not employed, so this value is changed from 365,243 to 0. 

```{r remove outliers}
#filter to remove incomes more than 100million
hc_train |> filter(amt_income_total <= 100000000) -> hc_train
#mutate days employed to 0 for positive values
hc_train |> mutate(days_employed = ifelse(days_employed > 0,0,days_employed)) -> hc_train
```



### Joining Data

At this point all five data sets were merged based on the customer id. To account for customers who were missing data points, which we would expect as we know customers don’t have information in each data set, we followed a basic approach for all independent variables. For each column a new column was created that was a flag that would indicate if data for that variable was missing for a given client, this way information about missing data was retained as this in and of itself is likely predictive.  Next imputation was performed for all missing data points; a median value was used for numeric variables and modal values for non-numeric. For numeric data they were all scaled and centered, as several different modeling approaches explored are sensitive to scale differences. These data transformation steps led to a high dimensionality of data, this along with numeric scaling make challenges in creating an interpretable model, but this trade off was made given that most of the models explored in the future will be black box, so interpretability will already be lost. 

```{r merge data}
#join data
hc_merged <- hc_train |> left_join(bureau_agg, by = "sk_id_curr")
hc_merged <- hc_merged |> left_join(installment_agg, by = "sk_id_curr")
hc_merged <- hc_merged |> left_join(credit_card_agg, by = "sk_id_curr")
hc_merged <- hc_merged |> left_join(pos_cash_agg, by = "sk_id_curr")

```


### Training and Test Partions
Data is split into a 80/20 train/test partition split. The 20% holdout data will only be used at the end for evaluating the estimated metrics and performance of the chosen model. Which model is used will be determined based on cross validation of the 80% training split data.  

```{r create partition }
set.seed(61)
#create data partition index
index <- createDataPartition(y = hc_merged$target, p = 0.8, list = FALSE)
#create split data frames
train_hc_merged <- hc_merged[index,]
holdout_hc_merged <- hc_merged[-index,] # do not use at all - only for very last run once model picked

```


### Missing Data Imputation and Log Scaling
Imputation of missing values and NAs is done last after data is all joined and split to avoid leakage. Income is log transformed due to it's heavy right skew. Modal values are imputed for categorical, median for numeric. Finally numeric data is scaled to account for certain models being sensitive to magnitude differences of variables. One additional item to note the median and modal values are calculated and retained from the training data, so for future predictions on holdout and test data these values are used for imputation. 

A data cleaning recipe is created that can be used on all test and training data. 
For training data an additional recipe is used that creates upsampled and downsampled data set for use. Because in the training data is there is a large amount of class imbalance with only ~8% of observations belonging to the minority class. For this reason, additional data sets were generated that used all the prior transformations but as a final step were up and down sampled to have more balance. For both we used a 2 to 1 ratio or two cases of majority class for each case in minority class. 

```{r recipe creation}
#recipe for data cleaning - to be used on all future data sets
rec <- recipe(target ~ ., data = train_hc_merged) |>
  update_role(sk_id_curr, new_role = "id") |> # set so ID doesn't get touched
  step_zv(all_predictors()) |>                # remove zero variance predictors
  
  # Flag NAs before anything else
  step_indicate_na(all_predictors()) |>
  
  # Convert logicals → numeric (0/1) so imputation will handle them
  step_mutate(across(where(is.logical), as.integer)) |>
  
  # Optional: sanitize numeric columns — replace Inf/-Inf/NaN with NA
  step_mutate(across(where(is.numeric), ~ ifelse(is.finite(.), ., NA_real_))) |>
  
  # Impute missing values (now covers numeric + logicals)
  step_impute_median(all_numeric_predictors()) |>
  step_impute_mode(all_nominal_predictors()) |>
  
  # Safe log transform (no negatives)
  step_mutate(amt_income_total = log1p(pmax(amt_income_total, 0))) |>
  
  # Normalize numeric predictors (except NA indicator flags)
  step_normalize(all_numeric_predictors(), -starts_with("na_ind_")) |>
  
  # Encode categoricals as one-hot
  step_dummy(all_nominal_predictors(), one_hot = TRUE) |>
  
  # Final cleanup of zero variance columns post-dummying
  step_zv(all_predictors())
```

**Upsampling techniques evaluated:**

- **SMOTE -** synthetic minority oversampling - creates synthetic nearest neighbor points (two big weaknesses of high n dimensions and categorical data) - for this reason not used
- **Upsample -** randomly oversampling with replacement, most simple and straight forward
- **ROSE -** random oversampling examples - creates synthetic data points using kernel density estimation (small item is it can create nonsensical examples, no other cons except computationally intensive and sensitive to tuning) - for this reason not used but could be revisited.

```{r over sample recipe}
# baseline recipe for no over sampling
rec_base  <- rec
#recipe for oversampling - adds oversampling at the end of data prep
rec_over_sample <- rec |> step_upsample(target, over_ratio = 0.5, skip = TRUE)
#recipe for downsample 
rec_down_sample <- rec |>
  step_downsample(target, under_ratio = 2, skip = TRUE)

```


```{r apply recipe to data}
#train recipe on training data split
prep_base <- prep(rec_base, training = train_hc_merged, verbose = FALSE)
prep_over <- prep(rec_over_sample, training = train_hc_merged, verbose = FALSE)
prep_down <- prep(rec_down_sample, training = train_hc_merged, verbose = FALSE)
#create data frames
x_train_base <- juice(prep_base)
x_train_over_sample <- juice(prep_over)
x_train_down_sample <- juice(prep_down)

```

# Modeling

## Benchmark Logistic Regression Model

We start with a null logistic regression that includes only an intercept (no predictors). This model predicts everyone’s default probability as the overall average in the training data. It serves as our baseline to compare more complex models. If future models don’t improve accuracy or AUC beyond this benchmark, it means the added predictors don’t add real value. In addition we trained a logistic regression using both up sampled and regular data sets with a few selected variables.

```{r}
# Logistic Regression Modeling

# Benchmark Model
accuracy <- yardstick::accuracy
roc_auc <- yardstick::roc_auc

#Benchmark Model-Logistic Regression
#This is the baseline Benchmark
null_model <- glm(target ~ 1, data = x_train_base, family = "binomial")
pred_null <- predict(null_model, type = "response")

#Creates a table with the actual labels, predicted probabilities and predicted classes and predicted classes (.pred_class, 1 if ≥ 0.5, else 0).
null_tbl <- tibble(
  truth = factor(x_train_base$target, levels = c(0, 1)),
  .pred = pred_null,
  .pred_class = factor(ifelse(pred_null >= 0.5, 1, 0), levels = c(0, 1))
)

#Calculate Accuracy and ROC AUC
# Define the metric set
bench_metrics <- metric_set(accuracy, roc_auc)

benchmark_results <- metric_set(accuracy, roc_auc)(null_tbl, truth = truth, estimate = .pred_class, .pred)
benchmark_results$model <- "Null (Intercept Only)"

# Logistic Regression Model (Selected Predictors)
#Predicts the probability that each applicant defaults
log_model <- glm(
  target ~ amt_income_total + days_employed +
    ext_source_1 + ext_source_2 + ext_source_3 +
    bur_n_loans + bur_sum_credit_sum +
    in_share_paid_late + in_n_missed_pmt +
    cc_util_mean + cc_share_dpd + cc_max_dpd +
    pos_share_paid_mean + pos_share_dpd + pos_max_dpd,
  data = x_train_base,
  family = binomial
)

pred_log <- predict(log_model, newdata = x_train_base, type = "response")

log_tbl <- tibble(
  truth = factor(x_train_base$target, levels = c(0, 1)),
  .pred = pred_log,
  .pred_class = factor(ifelse(pred_log >= 0.5, 1, 0), levels = c(0, 1))
)

log_results <- metric_set(accuracy, roc_auc)(log_tbl, truth = truth, estimate = .pred_class, .pred)
log_results$model <- "Logistic Regression (Selected Vars)"

# Logistic Regression Model (Oversampled Data)
#Same model formula, but now trained on the oversampled data (x_train_over_sample).To handle class imbalance
log_over <- glm(
  target ~ amt_income_total + days_employed +
    ext_source_1 + ext_source_2 + ext_source_3 +
    bur_n_loans + bur_sum_credit_sum +
    in_share_paid_late + in_n_missed_pmt +
    cc_util_mean + cc_share_dpd + cc_max_dpd +
    pos_share_paid_mean + pos_share_dpd + pos_max_dpd,
  data = x_train_over_sample,
  family = binomial
)

pred_over <- predict(log_over, newdata = x_train_over_sample, type = "response")

over_tbl <- tibble(
  truth = factor(x_train_over_sample$target, levels = c(0, 1)),
  .pred = pred_over,
  .pred_class = factor(ifelse(pred_over >= 0.5, 1, 0), levels = c(0, 1))
)

over_results <- metric_set(accuracy, roc_auc)(over_tbl, truth = truth, estimate = .pred_class, .pred)
over_results$model <- "Logistic Regression (Oversampled)"

# Summary Results
final_results <- bind_rows(benchmark_results, log_results, over_results) |>
  select(model, .metric, .estimate) |>
  tidyr::pivot_wider(names_from = .metric, values_from = .estimate) |>
  arrange(desc(roc_auc))

print(final_results)
```


#### Baseline (Null Model)

- The benchmark model only predicts the overall average default rate (no customer information is used).
- It provides a baseline accuracy and AUC of 0.5, representing random guessing.
- Purpose: establish a “zero-information” reference to measure improvement from more advanced models.

#### Logistic Regression (Selected Predictors)

- A standard logistic regression was trained using the key financial and behavioral predictors (income, employment, credit history, payment behavior, and external credit scores).
- This model slightly improves over the null baseline by incorporating applicant-level data.
- It shows moderate predictive lift, but class imbalance (few defaults) limits its performance.

#### Logistic Regression (Oversampled Data)

- To address the fact that default cases are rare, we re-trained the logistic model on oversampled data, giving more weight to minority (default) cases.
- Oversampling helps the model better identify potential defaulters.
- This adjustment improved recall for risky clients without materially hurting overall accuracy.
- However, results still show room for improvement in distinguishing high-risk from low-risk borrowers.

Given the number of variables present in our data set, it's impractical to try and specify the variables to use, so future modeling will make use of techniques that utilize built in feature selection. 

## Lasso Regression

```{r, echo = FALSE, warning = FALSE}
#LASSO using prepped data (x_train_base / baked holdout)

# Build X/y from the juiced training data (no NAs, all numeric)
y_tr <- ifelse(x_train_base$target == "1", 1, 0)

# Remove target and any pure ID columns if present
X_tr <- x_train_base %>%
  select(-target, -any_of(c("sk_id_curr"))) %>%
  as.matrix()

stopifnot(length(y_tr) == nrow(X_tr))

# 3-fold CV to pick lambda by AUC
set.seed(123)
cv_lasso <- cv.glmnet(
  x = X_tr,
  y = y_tr,
  family = "binomial",
  alpha = 1,            # LASSO
  nfolds = 3,
  type.measure = "auc"  # maximize mean AUC
)

lambda_min <- cv_lasso$lambda.min   # best AUC
lambda_1se <- cv_lasso$lambda.1se   # simpler (sparser) within 1 SE

#Fit final model on all training data (choose one)
fit_lasso <- glmnet(X_tr, y_tr, family = "binomial", alpha = 1, lambda = lambda_1se)

# Evaluate on your HOLDOUT (bake it with the SAME recipe)
x_test_base <- bake(prep_base, new_data = holdout_hc_merged)

y_te <- ifelse(x_test_base$target == "1", 1, 0)
X_te <- x_test_base %>%
  select(-target, -any_of(c("sk_id_curr"))) %>%
  as.matrix()

prob_te <- as.numeric(predict(fit_lasso, newx = X_te, type = "response"))

```

```{r lasso, echo = FALSE, warning = FALSE}
# See chosen lambdas
lambda_min
lambda_1se

# See CV AUC results (the built-in plot shows mean AUC per lambda)
plot(cv_lasso)

# Print the final LASSO model summary
fit_lasso

# Check which predictors were selected (nonzero coefficients)
coef(fit_lasso)[coef(fit_lasso) != 0]

# If you want a quick AUC score on your holdout:
library(pROC)
auc_holdout <- pROC::auc(response = y_te, predictor = prob_te)
auc_holdout

# Convert predicted probabilities to class labels using 0.5 threshold
pred_class <- ifelse(prob_te >= 0.5, 1, 0)

# Compare predicted vs actual labels
conf_matrix <- table(Predicted = pred_class, Actual = y_te)
conf_matrix

# Calculate accuracy manually
accuracy <- mean(pred_class == y_te)
accuracy

```

### Lasso Model Interpretation

The LASSO logistic regression model represented a significant advancement over the baseline and traditional logistic models by automatically selecting the most relevant predictors from a large pool of financial and behavioral variables. Using cross-validation to determine the optimal level of regularization (λ₁se = 0.000547), the final model retained 195 meaningful features while avoiding overfitting.

On the holdout dataset, the model achieved an AUC of 0.76, indicating strong ability to distinguish between clients likely and unlikely to default, and an overall accuracy of 91.9%. The confusion matrix confirms that the model correctly identified most non-defaulting applicants (56,448 true negatives) while minimizing false alarms (only 89 false positives). 

Although a smaller number of defaulters were missed (4,862 false negatives), the model still captured the majority of high-risk cases with 102 true positives. These results demonstrate that the LASSO model effectively balances precision and interpretability, providing a robust foundation for predicting default risk and supporting more data-driven credit decisions.

- Selected variables: The model retained 195 features, highlighting nuanced patterns in applicant financial history and external credit behavior.
- Holdout AUC = 0.76, indicating strong discriminative power — the model correctly ranks defaulters above non-defaulters roughly 76% of the time.
- Accuracy = 91.9%, meaning the model correctly predicts outcomes for about 9 out of 10 applicants.

**Confusion Matrix insight:**

- True negatives (good applicants correctly identified): 56,448

- False positives (good applicants incorrectly flagged): 89

- False negatives (missed defaulters): 4,862

- True positives (defaulters correctly caught): 102

**Key Takeaways**
The LASSO model outperforms all previous baselines, achieving the best trade-off between accuracy and interpretability. Even though defaults are rare, the model successfully identifies most high-risk customers, offering actionable insight for credit approval and risk monitoring. This model can serve as a production-ready risk scoring system, guiding more informed lending decisions while minimizing false approvals.

